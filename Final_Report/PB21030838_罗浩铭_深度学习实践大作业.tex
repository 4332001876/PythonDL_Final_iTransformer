
\documentclass[twoside,12pt]{article}
%\documentclass[UTF8]{ctexart}
\usepackage[heading=true]{ctex}

\RequirePackage{natbib}
% modification to natbib citations
\setcitestyle{authoryear,round,citesep={;},aysep={,},yysep={;}}

\usepackage{booktabs}
\usepackage{threeparttable}
\usepackage{multirow}

\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\usepackage{fancyhdr} % 页眉页脚
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{geometry}

\geometry{
  paper      = a4paper,
  vmargin    = 2.54cm,
  hmargin    = 3.17cm,
  headheight = 0.75cm,
  headsep    = 0.29cm,
  footskip   = 0.79cm,
}

\newcommand{\update}[1]{{\textcolor{black}{#1}}}
\newcommand{\boldres}[1]{{\textbf{\textcolor{red}{#1}}}}
\newcommand{\secondres}[1]{{\underline{\textcolor{blue}{#1}}}}

\pagestyle{fancy}

%\firstpageno{1}

\title{ }

\author{罗浩铭\ PB21030838}


\begin{document}

\fancyhf{} % 清除所有页眉页脚
\fancyfoot[C]{\thepage} % 设置右页脚为页码
\fancyhead[l]{\footnotesize  }
% 设置右页眉为章节标题 

\renewcommand{\headrulewidth}{0pt} % 去页眉线

\begin{center}
  \textbf{\LARGE{深度学习实践大作业——iTransformer}}\\
  \vspace{0.2cm}
  \large{罗浩铭\ PB21030838}
\end{center}
% 无错别字，语句通顺，格式整齐，排版良好  （5分）

\section{文章基本信息}

\begin{itemize}
  \item 论文题目：iTransformer: Inverted Transformers Are Effective for Time Series Forecasting~\citep{itransformer}
  \item 论文作者：Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, Mingsheng Long
  \item 论文来源：ICLR 2024(在投，但在OpenReview已经拿到3个8分和一个6分，基本确定中会，参见https://openreview.net/forum?id=JePfAI8fah)
  \item 首发日期：2023年10月10日
  \item 论文链接：\url{https://arxiv.org/abs/2310.06625}
  \item 论文官方实现：\url{https://github.com/thuml/iTransformer}（截至本报告完成时，已获得443颗星）
\end{itemize}



\section{文章介绍及理解}
% 简述对文章的理解   300-600字     （10分）

本文提出的iTransformer，即inverted Transformer，是一种新的基于Transformer~\citep{Transformer}的时间序列预测架构，它的做法其实非常简单：传统的Transformer架构是将同一时间点的各变量建模为一个token (temporal token)，对变量维进行FFN计算，并对时间轴进行attention计算；而iTransformer则是将同一变量的序列建模为一个token (variate token)，对时间维进行FFN计算提取时序特征，并对变量轴进行attention计算从而提取各变量之间的关联。也就是说，它倒置了传统Transformer架构中对时间轴和变量轴的处理方式，这也是其名字由来。正是这样的简单改进，使得其在不修改任何底层模块的条件下，大幅提升了模型性能，使之在各大复杂时序预测任务中取得了大幅领先的SOTA效果。

虽然其做法简单，但是其背后蕴含着深刻的思想。

首先，我们先来看传统Transformer架构的预测器为何表现不佳：
\begin{itemize}
  \item 一般而言，不同的变量具有完全不同的物理含义，即使语义相同，其度量单位也可能完全不同。此外，由于测量延迟等原因，其实际产生的时刻并不一定是严格对齐的。由此，同一时刻的多个变量服从不同的分布，且可能带时间偏移，作为token缺乏语义，容易学到无意义的注意力图
  \item 原本独立的变量在该架构下被杂糅为无法区分的特征通道，这会提高学习变量之间关联信息的难度
  \item 每个特征表示只能反映一个时间戳的特征，感受野过小；模型处理长期序列输入时，由于$O(n^2)$的attention计算复杂度，计算开销大，并且随着历史观新测的输入变长，预测效果反而下降
\end{itemize}

而iTransformer架构的好处在于：
\begin{itemize}
  \item 自注意力模块在其中能十分自然地提取到变量之间的关联信息，从而更好地捕捉变量之间的相互作用机制。
  \item 由于时间特征通常有着不少类似的模式，FFN适合提取时间特征，并有助于提高模型的泛化性
  \item Transformer中的层归一化（LayerNorm）可以帮助消除变量之间分布差异。
  \item 由于该架构只是改变了Transformer模型的用法，因此该工作可拓展到任意的魔改Transformer架构上，适用性极广。
\end{itemize}

由此，iTransformer架构在不修改任何底层模块的条件下，大幅提升了模型性能，并成为不少架构改进的新思路。

\section{代码结构}
% 对代码结构进行描述 300-1000字    （10分）

代码的总体结构为：以\verb |run.py|为入口，其拉起experiments部分中的主类，主类再调用data\_provider、model（model再调用layers）、utils等部分的类，完成数据的读取、模型的构建、训练、测试等过程。

下面自顶向底分别介绍代码的各部分：
\begin{itemize}
  \item script: 用于运行训练和测试的脚本，其中定义好了论文里的各个实验的脚本代码，特别是提供了合适的训练参数，可以直接运行。
  \item \verb |run.py|：整个程序的入口，用于接收命令行参数，并根据命令行参数，选择合适的实验类（是否是测试从部分变量的序列预测出全部变量的序列的实验），初始化后调用其训练、测试或预测函数，完成该次实验内容。
  \item model: 里面定义了Transformer, iTransformer，以及Transformer变体（包括Reformer~\citep{kitaev2020reformer}, Informer~\citep{Informer}, Flowformer~\citep{wu2022flowformer}, Flashformer~\citep{dao2022flashattention}）及其inverted版本。
  \item layers: 包含上述模型中的各个子模块，如attention及其各种改型, FFN, encoder, decoder, embedding (含原版的embedding方案和inverted的方案)等。
  \item data\_provider: 用于根据实验配置信息，读取各个数据集的数据，并提供相应的Data Loader。
  \item utils: 包含各种工具函数和类，如计算各种评价指标、loss的函数，提供attention mask的类、时间序列处理工具类及工具函数、学习率调节、早停等。
\end{itemize}

从Transformer到iTransformer的改动相当简单，只需要将Embedding层由\verb |DataEmbedding|换为\verb |DataEmbedding_inverted|，并相应地用permute调换维度，调整维度大小即可。

\section{训练及测试过程}
% 对训练和测试过程进行描述（各种超参），需含loss曲线的展示，对时间的描述等   500-1000字	（15分）

我们同时使用百度的aistudio（每日4小时额度，单卡V100）和kaggle平台（每周30小时额度，单卡P100）进行训练。

测试所用数据集与论文一致，如表\ref{tab:dataset}所示。

\begin{table}[thbp]
  \centering
  \vspace{0pt}
  \caption{数据集描述，\emph{Dim}代表时间序列变量数，\emph{Dataset Size}以 (Train, Validation, Test)的形式表示，\emph{Prediction Length}表示各预测序列长度的设定，\emph{Frequency} 表示时间序列的采样时间间隔}\label{tab:dataset}
  \vskip 0.15in
  \resizebox{\textwidth}{!}
  {
    \begin{threeparttable}
      \begin{small}
        \renewcommand{\multirowsetup}{\centering}
        \setlength{\tabcolsep}{4.5pt}
        \begin{tabular}{l|c|c|c|c|c}
          \toprule
          Dataset               & Dim & Prediction Length                     & Dataset Size                          & Frequency & Information                    \\
          \toprule
          \update{ETTh1, ETTh2} & 7   & \scalebox{0.8}{\{96, 192, 336, 720\}} & \scalebox{0.8}{(8545, 2881, 2881)}    & Hourly    & \scalebox{1.0}{Electricity}    \\
          \midrule
          \update{ETTm1, ETTm2} & 7   & \scalebox{0.8}{\{96, 192, 336, 720\}} & \scalebox{0.8}{(34465, 11521, 11521)} & 15min     & \scalebox{1.0}{Electricity}    \\
          \midrule
          \update{Exchange}     & 8   & \scalebox{0.8}{\{96, 192, 336, 720\}} & \scalebox{0.8}{(5120, 665, 1422)}     & Daily     & \scalebox{1.0}{Economy}        \\
          \midrule
          Weather               & 21  & \scalebox{0.8}{\{96, 192, 336, 720\}} & \scalebox{0.8}{(36792, 5271, 10540)}  & 10min     & \scalebox{1.0}{Weather}        \\
          \midrule
          ECL                   & 321 & \scalebox{0.8}{\{96, 192, 336, 720\}} & \scalebox{0.8}{(18317, 2633, 5261)}   & Hourly    & \scalebox{1.0}{Electricity}    \\
          \midrule
          Traffic               & 862 & \scalebox{0.8}{\{96, 192, 336, 720\}} & \scalebox{0.8}{(12185, 1757, 3509)}   & Hourly    & \scalebox{1.0}{Transportation} \\
          \midrule
          Solar-Energy          & 137 & \scalebox{0.8}{\{96, 192, 336, 720\}} & \scalebox{0.8}{(36601, 5161, 10417)}  & 10min     & \scalebox{1.0}{Energy}         \\
          \midrule
          \update{PEMS03}       & 358 & \scalebox{0.8}{\{12, 24, 48, 96\}}    & \scalebox{0.8}{(15617,5135,5135)}     & 5min      & \scalebox{1.0}{Transportation} \\
          \midrule
          \update{PEMS04}       & 307 & \scalebox{0.8}{\{12, 24, 48, 96\}}    & \scalebox{0.8}{(10172,3375,281)}      & 5min      & \scalebox{1.0}{Transportation} \\
          \midrule
          \update{PEMS07}       & 883 & \scalebox{0.8}{\{12, 24, 48, 96\}}    & \scalebox{0.8}{(16911,5622,468)}      & 5min      & \scalebox{1.0}{Transportation} \\
          \midrule
          \update{PEMS08}       & 170 & \scalebox{0.8}{\{12, 24, 48, 96\}}    & \scalebox{0.8}{(10690,3548,265)}      & 5min      & \scalebox{1.0}{Transportation} \\
          \bottomrule
        \end{tabular}
      \end{small}
    \end{threeparttable}
  }
  \vspace{0pt}
\end{table}

\section{复现结果}
% 复现结果与文章所示结果的对比，并分析结果不同的可能的原因  500-1000字    （15分）
\subsection{模型效果测试}
\begin{table}[htbp]
  \caption{}
  \label{tab:ablation}
  \vspace{5pt}
  \centering
  \resizebox{\textwidth}{!}
  {
    \begin{small}
      \renewcommand{\multirowsetup}{\centering}
      \setlength{\tabcolsep}{5.3pt}
      \begin{tabular}{c|c|cc|cc|cc|cc|cc}
        \toprule
        \multirow{2}{*}{Models}                                  & Prediction & \multicolumn{2}{c|}{ECL} & \multicolumn{2}{c|}{Exchange} & \multicolumn{2}{c|}{Traffic} & \multicolumn{2}{c|}{Weather} & \multicolumn{2}{c}{Solar-Energy}                                         \\
        \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12}
                                                                 & Length     & MSE                      & MAE                           & MSE                          & MAE                          & MSE                              & MAE   & MSE   & MAE   & MSE   & MAE   \\
        \midrule
        \multirow{5}{*}{\textbf{iTransformer (Our Replication)}} & 96         & 0.147                    & 0.239                         & 0.086                        & 0.206                        & 0.393                            & 0.269 & 0.176 & 0.216 & 0.208 & 0.243 \\
                                                                 & 192        & 0.168                    & 0.259                         & 0.180                        & 0.304                        & 0.413                            & 0.277 & 0.225 & 0.257 & 0.239 & 0.263 \\
                                                                 & 336        & 0.178                    & 0.270                         & 0.338                        & 0.422                        & 0.425                            & 0.283 & 0.281 & 0.299 & 0.251 & 0.275 \\
                                                                 & 720        & 0.210                    & 0.300                         & 0.853                        & 0.696                        & 0.459                            & 0.299 & 0.358 & 0.350 & 0.250 & 0.276 \\
        \cmidrule(lr){2-12}
                                                                 & Avg        & 0.176                    & 0.267                         & 0.364                        & 0.407                        & 0.423                            & 0.282 & 0.260 & 0.281 & 0.239 & 0.264 \\
        \midrule
        \multirow{5}{*}{\textbf{iTransformer}}                   & 96         & 0.148                    & 0.240                         & 0.086                        & 0.206                        & 0.395                            & 0.268 & 0.174 & 0.214 & 0.203 & 0.237 \\
                                                                 & 192        & 0.162                    & 0.253                         & 0.177                        & 0.299                        & 0.417                            & 0.276 & 0.221 & 0.254 & 0.233 & 0.261 \\
                                                                 & 336        & 0.178                    & 0.269                         & 0.331                        & 0.417                        & 0.433                            & 0.283 & 0.278 & 0.296 & 0.248 & 0.273 \\
                                                                 & 720        & 0.225                    & 0.317                         & 0.847                        & 0.691                        & 0.467                            & 0.302 & 0.358 & 0.349 & 0.249 & 0.275 \\
        \cmidrule(lr){2-12}
                                                                 & Avg        & 0.178                    & 0.270                         & 0.360                        & 0.403                        & 0.428                            & 0.282 & 0.258 & 0.279 & 0.233 & 0.262 \\
        \bottomrule
      \end{tabular}
    \end{small}
  }
  \resizebox{0.9\textwidth}{!}
  {
    \begin{small}
      \renewcommand{\multirowsetup}{\centering}
      \setlength{\tabcolsep}{5.3pt}
      \begin{tabular}{c|c|cc|cc|cc|cc}
        \toprule
        \multirow{2}{*}{Models}                                  & Prediction & \multicolumn{2}{c|}{ETTm1} & \multicolumn{2}{c|}{ETTm2} & \multicolumn{2}{c|}{ETTh1} & \multicolumn{2}{c|}{ETTh2}                                 \\
        \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10}
                                                                 & Length     & MSE                        & MAE                        & MSE                        & MAE                        & MSE   & MAE   & MSE   & MAE   \\
        \midrule
        \multirow{5}{*}{\textbf{iTransformer (Our Replication)}} & 96         & 0.342                      & 0.377                      & 0.186                      & 0.272                      & 0.387 & 0.405 & 0.301 & 0.350 \\
                                                                 & 192        & 0.383                      & 0.396                      & 0.254                      & 0.314                      & 0.441 & 0.436 & 0.380 & 0.399 \\
                                                                 & 336        & 0.418                      & 0.418                      & 0.316                      & 0.351                      & 0.491 & 0.462 & 0.424 & 0.432 \\
                                                                 & 720        & 0.487                      & 0.457                      & 0.414                      & 0.407                      & 0.509 & 0.494 & 0.430 & 0.447 \\
        \cmidrule(lr){2-10}
                                                                 & Avg        & 0.408                      & 0.412                      & 0.293                      & 0.336                      & 0.457 & 0.449 & 0.384 & 0.407 \\
        \midrule
        \multirow{5}{*}{\textbf{iTransformer}}                   & 96         & 0.334                      & 0.368                      & 0.180                      & 0.264                      & 0.386 & 0.405 & 0.297 & 0.349 \\
                                                                 & 192        & 0.377                      & 0.391                      & 0.250                      & 0.309                      & 0.441 & 0.436 & 0.380 & 0.400 \\
                                                                 & 336        & 0.426                      & 0.420                      & 0.311                      & 0.348                      & 0.487 & 0.458 & 0.428 & 0.432 \\
                                                                 & 720        & 0.491                      & 0.459                      & 0.412                      & 0.407                      & 0.503 & 0.491 & 0.427 & 0.445 \\
        \cmidrule(lr){2-10}
                                                                 & Avg        & 0.407                      & 0.410                      & 0.288                      & 0.332                      & 0.454 & 0.447 & 0.383 & 0.407 \\
        \bottomrule
      \end{tabular}
    \end{small}
  }
  \resizebox{0.9\textwidth}{!}
  {
    \begin{small}
      \renewcommand{\multirowsetup}{\centering}
      \setlength{\tabcolsep}{5.3pt}
      \begin{tabular}{c|c|cc|cc|cc|cc}
        \toprule
        \multirow{2}{*}{Models}                                  & Prediction & \multicolumn{2}{c|}{PEMS03} & \multicolumn{2}{c|}{PEMS04} & \multicolumn{2}{c|}{PEMS07} & \multicolumn{2}{c|}{PEMS08}                                 \\
        \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10}
                                                                 & Length     & MSE                         & MAE                         & MSE                         & MAE                         & MSE   & MAE   & MSE   & MAE   \\
        \midrule
        \multirow{5}{*}{\textbf{iTransformer (Our Replication)}} & 12         & 0.068                       & 0.174                       & 0.080                       & 0.188                       & 0.067 & 0.165 & 0.088 & 0.193 \\
                                                                 & 24         & 0.098                       & 0.209                       & 0.099                       & 0.212                       & 0.087 & 0.190 & 0.138 & 0.243 \\
                                                                 & 48         & 0.167                       & 0.277                       & 0.129                       & 0.243                       & 1.047 & 0.864 & 0.262 & 0.296 \\
                                                                 & 96         & 1.014                       & 0.780                       & 0.170                       & 0.281                       & 1.039 & 0.855 & 0.298 & 0.327 \\
        \cmidrule(lr){2-10}
                                                                 & Avg        & 0.337                       & 0.360                       & 0.120                       & 0.231                       & 0.560 & 0.519 & 0.197 & 0.265 \\
        \midrule
        \multirow{5}{*}{\textbf{iTransformer}}                   & 12         & 0.071                       & 0.174                       & 0.078                       & 0.183                       & 0.067 & 0.165 & 0.079 & 0.182 \\
                                                                 & 24         & 0.093                       & 0.201                       & 0.095                       & 0.205                       & 0.088 & 0.190 & 0.115 & 0.219 \\
                                                                 & 48         & 0.125                       & 0.236                       & 0.120                       & 0.233                       & 0.110 & 0.215 & 0.186 & 0.235 \\
                                                                 & 96         & 0.164                       & 0.275                       & 0.150                       & 0.262                       & 0.139 & 0.245 & 0.221 & 0.267 \\
        \cmidrule(lr){2-10}
                                                                 & Avg        & 0.113                       & 0.221                       & 0.111                       & 0.221                       & 0.101 & 0.204 & 0.150 & 0.226 \\
        \bottomrule
      \end{tabular}
    \end{small}
  }
\end{table}


\subsection{不同超参对模型效果的影响}



\subsection{对优化效率的iFlashTransformer的测试}





\section{感想}
% 复现过程中的感悟、吐槽  （300-1000字） （5分）




\section{探索性修改}
% (非必须，奖励分) 对代码进行了探索性修改，描述修改经过，并报告实验结果   （5分）


\bibliography{dllab_final_report}
\bibliographystyle{iclr2024_conference}

\vfill

\end{document}
