
\documentclass[twoside,12pt]{article}
%\documentclass[UTF8]{ctexart}
\usepackage[heading=true]{ctex}

\RequirePackage{natbib}
% modification to natbib citations
\setcitestyle{authoryear,round,citesep={;},aysep={,},yysep={;}}

\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\usepackage{fancyhdr} % 页眉页脚
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{geometry}

\geometry{
  paper      = a4paper,
  vmargin    = 2.54cm,
  hmargin    = 3.17cm,
  headheight = 0.75cm,
  headsep    = 0.29cm,
  footskip   = 0.79cm,
}


\newcommand{\update}[1]{{\textcolor{black}{#1}}}
\newcommand{\boldres}[1]{{\textbf{\textcolor{red}{#1}}}}
\newcommand{\secondres}[1]{{\underline{\textcolor{blue}{#1}}}}

\pagestyle{fancy}

%\firstpageno{1}

\title{ }

\author{罗浩铭\ PB21030838}


\begin{document}

\fancyhf{} % 清除所有页眉页脚
\fancyfoot[C]{\thepage} % 设置右页脚为页码
\fancyhead[l]{\footnotesize  }
% 设置右页眉为章节标题 

\renewcommand{\headrulewidth}{0pt} % 去页眉线

\begin{center}
  \textbf{\LARGE{iTransformer论文翻译报告}}\\
  \vspace{0.2cm}
  \large{罗浩铭\ PB21030838}
\end{center}

% 找一篇2021年以后的知名会议和期刊上发表的人工智能相关的文章，翻译其摘要与Introduction部分，写一篇翻译报告（附上原文与翻译后的中文）
% 文章翻译正确，流畅，通顺，信达雅

\section{文章基本信息}

\begin{itemize}
  \item 论文题目：iTransformer: Inverted Transformers Are Effective for Time Series Forecasting~\citep{itransformer}
  \item 论文作者：Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, Mingsheng Long
  \item 论文来源：ICLR 2024(在投，但在OpenReview已经拿到3个8分和一个6分，基本确定中会，参见https://openreview.net/forum?id=JePfAI8fah)
  \item 首发日期：2023年10月10日
  \item 论文链接：\url{https://arxiv.org/abs/2310.06625}
  \item 论文官方实现：\url{https://github.com/thuml/iTransformer}（截至本报告完成时，已获得443颗星）
\end{itemize}

\section{摘要}
\subsection{译文}
近年来，线性时间序列预测模型的迅速兴起，对此前人们一直热衷的路线——改进基于Transformer的预测器架构——提出了质疑。这些预测器利用Transformer来将时间序列中同一时刻下的多个变量值建模为temporal tokens，由此来捕捉时间上的全局依赖关系。然而，Transformer架构的模型在处理更长的历史观测序列的输入时，会遭遇性能下降与计算量剧增的挑战。此外，对于每个temporal token融合的多个变量，它们可能具有不同物理意义，采集时间可能并不对齐，且尺度可能差异较大，这将使得模型无法学习到以变量为中心的表征，并使得模型容易学习到无意义的注意力图。在这项工作中，我们重新思考了Transformer组件在整个架构中的用途和地位，并在不修改任何底层组件的情况下提出了我们的新架构。我们提出了\textbf{iTransformer}，它简单地互换注意力机制和FFN作用的维度。具体来说，它将同一个变量的序列建模为variate tokens，并在此之上使用注意力机制来捕捉变量之间的联系；同时，它使用FFN对各variate tokens进行逐层编码，来获取其非线性表示，并最终解码为对未来序列的预测结果。我们的iTransformer架构在各大富有挑战的真实世界数据集中都取得了SOTA的效果，增强了整个Transformer系列的性能，提高了它们于不同变量间的泛化能力，并使得它们可以对任意长度的输入序列进行更好利用。这使得iTransformer成为时间序列预测任务中一个强有力的backbone竞争方案。

\subsection{原文}
The recent boom of linear forecasting models questions the ongoing passion for architectural modifications of Transformer-based forecasters. These forecasters leverage Transformers to model the global dependencies over \emph{temporal tokens} of time series, with each token formed by multiple variates of the same timestamp. However, Transformers are challenged in forecasting series with larger lookback windows due to performance degradation and computation explosion. Besides, the embedding for each temporal token fuses multiple variates \update{that represent potential delayed events and distinct physical measurements}, which may fail in learning variate-centric representations and result in meaningless attention maps. In this work, we reflect on the competent duties of Transformer components and repurpose the Transformer architecture without any modification to the basic components. We propose \textbf{iTransformer} that simply \update{applies the attention and feed-forward network on the inverted dimensions}. Specifically, the time points of individual series are embedded into \emph{variate tokens} which are utilized by the attention mechanism to capture multivariate correlations; meanwhile, the feed-forward network is applied for each variate token to learn nonlinear representations. The iTransformer model achieves state-of-the-art on challenging real-world datasets, which further empowers the Transformer family with promoted performance, generalization ability across different variates, and better utilization of arbitrary lookback windows, making it a nice alternative as the fundamental backbone of time series forecasting.

\section{Introduction部分}
\subsection{译文}
\begin{figure}[t]
  \begin{center}
    \includegraphics[width=0.5\columnwidth]{pic/radar.pdf}
    % \vspace{-5pt}
    \vspace{-10pt}
    \caption{\small{iTransformer性能雷达图。平均结果 (MSE) 按照TimesNet~\citeyearpar{Timesnet}中的方式测得}.}
    \label{fig:radar_cn}
  \end{center}
  % \vspace{-14pt}
  \vspace{-14pt}
\end{figure}

Transformer~\citep{Transformer}架构已经在自然语言处理~\citep{brown2020language}和计算机视觉~\citep{dosovitskiy2020image}领域取得了巨大的成功，成为了遵循scaling law~\citep{kaplan2020scaling}的基础模型。受其在广泛领域取得的巨大成功的启发，同时因其强大的刻画依赖关系和提取序列多层次表征的能力，Transformer在时间序列预测领域正在兴起。~\citep{Informer, Autoformer, PatchTST}

然而，研究者们最近开始质疑基于Transformer的预测模型的有效性，这些模型通常将同一时间戳的多个变量杂糅到无法区分的特征通道中成为temporal tokens，并在这些temporal tokens上应用注意力机制来捕捉时间上的依赖关系。考虑到各时刻之间变量的关系更多地是数值上的，而非语义上的，研究者们发现简单的线性层（可以追溯到基于统计的预测器~\citep{box1968some}）能够达到超过复杂的Transformer的性能与效率~\citep{DLinear, das2023long}。与此同时，最近的研究更加强调保证变量之间的独立性并使用它们之间的互信息，而非显式地对各变量之间的关系进行建模，以实现准确的预测~\citep{DLinear, das2023long}。但是如果不颠覆原来的Transformer架构，这一目标很难实现。

考虑到对基于Transformer的预测器的争议，我们反思了为何在其他领域表现出色的Transformer，却在时间序列预测任务中表现得甚至不如线性模型。我们注意到，基于Transformer的预测器的现有架构可能不适合用于多变量时间序列预测。如图~\ref{fig:motivation_cn}顶部所示，相同时刻的变量值实际上代表着完全不同的物理意义，并通常有着不同的测量单位，而这些变量值却被杂糅在一个token中，抹去了多变量之间的关联。而且，由单个时间步组成的token可能由于过小的感受野或同一时间点内混杂着不同时间的信息而难以提取出有效信息。此外，由于序列的变化与序列顺序息息相关，因此顺序无关的注意力机制不适合用于时间维度上~\citep{DLinear}。因此，Transformer在捕捉序列表征及描绘多变量间关联方面的能力较弱，限制了其在各种时间序列数据上的表现和泛化能力。

考虑到将一个时间戳内的多个变量值建模到一个token的潜在问题，我们采取了一个\emph{倒置的视角}来看待时间序列，并将每个变量的整条时间序列建模到一个token中，这相当于是Patching~\citep{PatchTST}扩大局部感受野后的极端情况。通过倒置，每一个token融合了序列的全局表征，从而使得表征更多地以变量为中心，并且使得可扩展性很强的注意力机制可以更好地捕捉多变量之间的关联。同时，前馈网络（FFN）在编码得到各个变量各种长度的历史序列的全局表征以及解码得到预测序列等方面有着足够好的表现。

基于上述的动机，我们认为：Transformer在时间序列预测任务中表现不佳，并不是因为Transformer本身无效，而是Transformer被不当使用。在本文中，我们重新审视了Transformer的结构，并提出了\emph{iTransformer}作为时间序列预测的backbone。从技术细节上来说，我们将每条时间序列建模为variate tokens，使用注意力机制来学习各变量之间的关联，并应用FFN来获取序列表征。从实验结果来看，我们提出的iTransformer在各真实世界的预测任务benchmark上取得了SOTA的表现（参见图~\ref{fig:radar_cn}），并且令人惊喜地解决了基于Transformer的预测器的痛点。我们的贡献有三个方面：


\begin{figure}[t]
  \begin{center}
    \includegraphics[width=0.95\columnwidth]{pic/motivation.pdf}
    % \vspace{-5pt}
    \vspace{-10pt}
    \caption{\small{\update{对原来的Transformer (上) 和我们提出的iTransformer (下) 的比较。}Transformer将每个时间步的多个变量值建模为temporal token，而iTransformer则将每个变量的序列独立地建模为variate token，从而其中的注意力模块将描述多变量之间的关联，而FFN对序列进行编码表征}}
    \label{fig:motivation_cn}
  \end{center}
  % \vspace{-14pt}
  \vspace{-14pt}
\end{figure}



\begin{itemize}
  \item 我们反思了Transformer的架构，并且认为原生的Transformer组件在处理多变量时间序列上的能力还没有被充分挖掘。
  \item 我们提出了iTransformer，它将独立的时间序列建模为token，使用注意力机制来捕捉多变量之间的关联，并使用层归一化和前馈网络模块来学习更好的序列表征。
  \item 从实验结果上来看，iTransformer在各真实世界的预测任务benchmark上取得了SOTA的表现，并且我们对倒置的模块和架构选择进行了广泛的分析，指明了基于Transformer的预测器在未来改进中的一个有希望的方向。
\end{itemize}

\subsection{原文}

\begin{figure}[t]
  \begin{center}
    \includegraphics[width=0.5\columnwidth]{pic/radar.pdf}
    % \vspace{-5pt}
    \vspace{-10pt}
    \caption{\small{Performance of iTransformer. Average results (MSE) are reported following TimesNet~\citeyearpar{Timesnet}}.}
    \label{fig:radar}
  \end{center}
  % \vspace{-14pt}
  \vspace{-14pt}
\end{figure}


Transformer~\citep{Transformer} has achieved tremendous success in natural language processing~\citep{brown2020language} and computer vision~\citep{dosovitskiy2020image}, growing into the foundation model that follows the scaling law~\citep{kaplan2020scaling}. Inspired by the immense success in extensive fields, Transformer with strong capabilities of depicting pairwise dependencies and extracting multi-level representations in sequences is emerging in time series forecasting~\citep{Informer, Autoformer, PatchTST}.


However, researchers have recently begun to question the validity of Transformer-based forecasters, which typically embed multiple variates of the same timestamp into indistinguishable channels and apply attention on these \emph{temporal tokens} to capture temporal dependencies. Considering the numerical but less semantic relationship among time points, researchers find that simple linear layers, which can be traced back to statistical forecasters~\citep{box1968some}, have exceeded complicated Transformers on both performance and efficiency~\citep{DLinear, das2023long}. Meanwhile, ensuring the independence of variate and utilizing mutual information is ever more highlighted by recent research that explicitly models multivariate correlations to achieve accurate forecasting~\citep{Crossformer, TSMixer}, but this goal can be hardly achieved without subverting the vanilla Transformer architecture.


Considering the disputes of Transformer-based forecasters, we reflect on why Transformers perform even worse than linear models in time series forecasting while acting predominantly in many other fields. We notice that the existing structure of Transformer-based forecasters may be not suitable for multivariate time series forecasting. As shown on the top of Figure~\ref{fig:motivation}, it is notable that the points of the same time step that basically represent completely different physical meanings recorded by inconsistent measurements are embedded into one token with wiped-out multivariate correlations. And the token formed by a single time step can struggle to reveal beneficial information due to excessively local receptive field and \update{time-unaligned events represented by simultaneous time points}. Besides, while series variations can be greatly influenced by the sequence order, permutation-invariant attention mechanisms are improperly adopted on the temporal dimension~\citep{DLinear}. Consequently, Transformer is weakened to capture essential series representations and portray multivariate correlations, limiting its capacity and generalization ability on diverse time series data.


\update{Concerning the potential risks of embedding multivariate points of a timestamp as a (temporal) token}, we take an \emph{inverted view} on time series and embed the whole time series of each variate independently into a (variate) token, the extreme case of Patching~\citep{PatchTST} that enlarges local receptive field. By inverting, the embedded token aggregates the global representations of series that can be more variate-centric and better leveraged by booming attention mechanisms for multivariate correlating. Meanwhile, the feed-forward network can be proficient enough to learn generalizable representations for distinct variates encoded from arbitrary lookback series and decoded to predict future series.

Based on the above motivations, we believe it is not that Transformer is ineffective for time series forecasting, but rather it is improperly used. In this paper, we revisit the structure of Transformer and advocate \emph{iTransformer} as a fundamental backbone for time series forecasting. Technically, we embed each time series as \emph{variate tokens}, adopt the attention for multivariate correlations, and employ the feed-forward network for series representations. Experimentally, the proposed iTransformer achieves state-of-the-art performance on real-world forecasting benchmarks shown in Figure~\ref{fig:radar} and surprisingly tackles the pain points of  Transformer-based forecasters. Our contributions lie in three aspects:


\begin{figure}[t]
  \begin{center}
    \includegraphics[width=0.95\columnwidth]{pic/motivation.pdf}
    % \vspace{-5pt}
    \vspace{-10pt}
    \caption{\small{\update{Comparison between the vanilla Transformer (top) and the proposed iTransformer (bottom).} Transformer embeds the temporal token, which contains the multivariate representation of each time step. iTransformer embeds each series independently to the variate token, such that the attention module depicts the multivariate correlations and the feed-forward network encodes series representations.}}
    \label{fig:motivation}
  \end{center}
  % \vspace{-14pt}
  \vspace{-14pt}
\end{figure}

\begin{itemize}
  \item We reflect on the architecture of Transformer and refine that the competent capability of native Transformer components on multivariate time series is underexplored.
  \item We propose iTransformer that regards independent time series as tokens to capture multivariate correlations by self-attention and utilize layer normalization and feed-forward network modules to learn better series-global representations for time series forecasting.
  \item Experimentally, iTransformer achieves comprehensive state-of-the-art on real-world benchmarks. We extensively analyze the inverted modules and architecture choices, indicating a promising direction for the future improvement of Transformer-based forecasters.
\end{itemize}

\bibliography{dllab_final_report}
\bibliographystyle{iclr2024_conference}

\vfill

\end{document}
