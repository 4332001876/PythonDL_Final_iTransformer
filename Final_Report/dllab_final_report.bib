@misc{itransformer,
  title         = {iTransformer: Inverted Transformers Are Effective for Time Series Forecasting},
  author        = {Yong Liu and Tengge Hu and Haoran Zhang and Haixu Wu and Shiyu Wang and Lintao Ma and Mingsheng Long},
  year          = {2023},
  eprint        = {2310.06625},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@article{Timesnet,
  title   = {TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis},
  author  = {Wu, Haixu and Hu, Tengge and Liu, Yong and Zhou, Hang and Wang, Jianmin and Long, Mingsheng},
  journal = {ICLR},
  year    = {2023}
}

@article{PatchTST,
  title   = {A Time Series is Worth 64 Words: Long-term Forecasting with Transformers},
  author  = {Nie, Yuqi and Nguyen, Nam H and Sinthong, Phanwadee and Kalagnanam, Jayant},
  journal = {ICLR},
  year    = {2023}
}

@article{Crossformer,
  title   = {Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting},
  author  = {Zhang, Yunhao and Yan, Junchi},
  journal = {ICLR},
  year    = {2023}
}

@article{SCINet,
  title   = {SCINet: time series modeling and forecasting with sample convolution and interaction},
  author  = {Liu, Minhao and Zeng, Ailing and Chen, Muxi and Xu, Zhijian and Lai, Qiuxia and Ma, Lingna and Xu, Qiang},
  journal = {NeurIPS},
  year    = {2022}
}

@article{Autoformer,
  title   = {Autoformer: Decomposition Transformers with {Auto-Correlation} for Long-Term Series Forecasting},
  author  = {Haixu Wu and Jiehui Xu and Jianmin Wang and Mingsheng Long},
  journal = {NeurIPS},
  year    = {2021}
}

@article{Informer,
  title   = {Informer: Beyond efficient transformer for long sequence time-series forecasting},
  author  = {Li, Jianxin and Hui, Xiong and Zhang, Wancai},
  journal = {arXiv: 2012.07436},
  year    = {2021}
}


@article{Unet,
  title        = {U-net: Convolutional networks for biomedical image segmentation},
  author       = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  journal      = {Medical Image Computing and Computer-Assisted Intervention--MICCAI 2015: 18th International Conference},
  pages        = {234--241},
  year         = {2015},
  organization = {Springer}
}

@article{Pyraformer,
  title   = {Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting},
  author  = {Liu, Shizhan and Yu, Hang and Liao, Cong and Li, Jianguo and Lin, Weiyao and Liu, Alex X and Dustdar, Schahram},
  journal = {International conference on learning representations},
  year    = {2021}
}

@article{DSTP-RNN,
  title     = {DSTP-RNN: A dual-stage two-phase attention-based recurrent neural network for long-term and multivariate time series prediction},
  author    = {Liu, Yeqi and Gong, Chuanyang and Yang, Ling and Chen, Yingyi},
  journal   = {Expert Systems with Applications},
  volume    = {143},
  pages     = {113082},
  year      = {2020},
  publisher = {Elsevier}
}

@article{Stationary,
  title   = {Non-stationary Transformers: Rethinking the Stationarity in Time Series Forecasting},
  author  = {Yong Liu and Haixu Wu and Jianmin Wang and Mingsheng Long},
  journal = {NeurIPS},
  year    = {2022}
}

@article{fedformer,
  title   = {{FEDformer}: Frequency enhanced decomposed transformer for long-term series forecasting},
  author  = {Zhou, Tian and Ma, Ziqing and Wen, Qingsong and Wang, Xue and Sun, Liang and Jin, Rong},
  journal = {ICML},
  year    = {2022}
}

@article{DLinear,
  title   = {Are Transformers Effective for Time Series Forecasting?},
  author  = {Ailing Zeng and Muxi Chen and Lei Zhang and Qiang Xu},
  journal = {AAAI},
  year    = {2023}
}

@article{Nbeats,
  title   = {N-{BEATS}: Neural basis expansion analysis for interpretable time series forecasting},
  author  = {Oreshkin, Boris N and Carpov, Dmitri and Chapados, Nicolas and Bengio, Yoshua},
  journal = {ICLR},
  year    = {2019}
}

@article{Transformer,
  author  = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  journal = {NeurIPS},
  title   = {Attention is All you Need},
  year    = {2017}
}

@article{LayerNorm,
  author  = {Ba, Jimmy Lei  and  Kiros, Jamie Ryan  and  Hinton, Geoffrey E.},
  journal = {https://arxiv.org/pdf/1607.06450.pdf},
  title   = {Layer Normalization},
  year    = {2016}
}

@article{Pytorch,
  title   = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author  = {Adam Paszke and S. Gross and Francisco Massa and A. Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Z. Lin and N. Gimelshein and L. Antiga and Alban Desmaison and Andreas K{\"o}pf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
  journal = {NeurIPS},
  year    = {2019}
}

@article{Adam,
  author  = {Diederik P. Kingma and
             Jimmy Ba},
  title   = {Adam: {A} Method for Stochastic Optimization},
  journal = {ICLR},
  year    = {2015}
}

@article{Nhits,
  title   = {N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting},
  author  = {Challu, Cristian and Olivares, Kin G and Oreshkin, Boris N and Garza, Federico and Mergenthaler, Max and Dubrawski, Artur},
  journal = {arXiv preprint arXiv:2201.12886},
  year    = {2022}
}

@article{salinas2019high,
  title   = {High-dimensional multivariate forecasting with low-rank gaussian copula processes},
  author  = {Salinas, David and Bohlke-Schneider, Michael and Callot, Laurent and Medico, Roberto and Gasthaus, Jan},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {32},
  year    = {2019}
}

@article{RNN1,
  title   = {Deep state space models for time series forecasting},
  author  = {Syama Sundar Rangapuram and Matthias W Seeger and Jan Gasthaus and Lorenzo Stella and Yuyang Wang and Tim Januschowski},
  journal = {NeurIPS},
  year    = {2018}
}

@article{LSTNet,
  title   = {Modeling long-and short-term temporal patterns with deep neural networks},
  author  = {Lai, Guokun and Chang, Wei-Cheng and Yang, Yiming and Liu, Hanxiao},
  journal = {SIGIR},
  year    = {2018}
}

@article{LSTMnetwork,
  title     = {LSTM network: a deep learning approach for short-term traffic forecast},
  author    = {Zhao, Zheng and Chen, Weihai and Wu, Xingming and Chen, Peter CY and Liu, Jingmeng},
  journal   = {IET Intelligent Transport Systems},
  volume    = {11},
  number    = {2},
  pages     = {68--75},
  year      = {2017},
  publisher = {Wiley Online Library}
}

@article{brown2020language,
  title   = {Language models are few-shot learners},
  author  = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal = {NeurIPS},
  year    = {2020}
}

@article{dosovitskiy2020image,
  title   = {An image is worth 16x16 words: Transformers for image recognition at scale},
  author  = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal = {ICLR},
  year    = {2021}
}

@article{kaplan2020scaling,
  title   = {Scaling laws for neural language models},
  author  = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal = {arXiv preprint arXiv:2001.08361},
  year    = {2020}
}

@inproceedings{lee2019set,
  title        = {Set transformer: A framework for attention-based permutation-invariant neural networks},
  author       = {Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam and Choi, Seungjin and Teh, Yee Whye},
  booktitle    = {International conference on machine learning},
  pages        = {3744--3753},
  year         = {2019},
  organization = {PMLR}
}

@article{bai1803empirical,
  title   = {An empirical evaluation of generic convolutional and recurrent networks for sequence modeling.},
  author  = {Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
  journal = {arXiv preprint arXiv:1803.01271},
  volume  = {2},
  year    = {2018}
}

@article{TSMixer,
  title   = {TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting},
  author  = {Vijay Ekambaram and Arindam Jati and Nam Nguyen and Phanwadee Sinthong and Jayant Kalagnanam
             },
  journal = {KDD},
  year    = {2023}
}

@article{das2023long,
  title   = {Long-term Forecasting with TiDE: Time-series Dense Encoder},
  author  = {Das, Abhimanyu and Kong, Weihao and Leach, Andrew and Sen, Rajat and Yu, Rose},
  journal = {arXiv preprint arXiv:2304.08424},
  year    = {2023}
}

@article{box1968some,
  title     = {Some recent advances in forecasting and control},
  author    = {Box, George EP and Jenkins, Gwilym M},
  journal   = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
  volume    = {17},
  number    = {2},
  pages     = {91--109},
  year      = {1968},
  publisher = {JSTOR}
}

@article{devlin2018bert,
  title   = {Bert: Pre-training of deep bidirectional transformers for language understanding},
  author  = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal = {arXiv preprint arXiv:1810.04805},
  year    = {2018}
}


@article{kim2021reversible,
  title   = {Reversible instance normalization for accurate time-series forecasting against distribution shift},
  author  = {Kim, Taesung and Kim, Jinhee and Tae, Yunwon and Park, Cheonbok and Choi, Jang-Ho and Choo, Jaegul},
  journal = {ICLR},
  year    = {2021}
}

@article{hornik1991approximation,
  title     = {Approximation capabilities of multilayer feedforward networks},
  author    = {Hornik, Kurt},
  journal   = {Neural networks},
  volume    = {4},
  number    = {2},
  pages     = {251--257},
  year      = {1991},
  publisher = {Elsevier}
}

@article{li2023revisiting,
  title   = {Revisiting Long-term Time Series Forecasting: An Investigation on Linear Mapping},
  author  = {Li, Zhe and Qi, Shiyi and Li, Yiduo and Xu, Zenglin},
  journal = {arXiv preprint arXiv:2305.10721},
  year    = {2023}
}

@article{kitaev2020reformer,
  title   = {Reformer: The efficient transformer},
  author  = {Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  journal = {ICLR},
  year    = {2020}
}

@inproceedings{liu2021pyraformer,
  title     = {Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting},
  author    = {Liu, Shizhan and Yu, Hang and Liao, Cong and Li, Jianguo and Lin, Weiyao and Liu, Alex X and Dustdar, Schahram},
  booktitle = {International conference on learning representations},
  year      = {2021}
}

@article{wu2022flowformer,
  title   = {Flowformer: Linearizing transformers with conservation flows},
  author  = {Wu, Haixu and Wu, Jialong and Xu, Jiehui and Wang, Jianmin and Long, Mingsheng},
  journal = {ICML},
  year    = {2022}
}

@article{Kornblith2019SimilarityON,
  title   = {Similarity of Neural Network Representations Revisited},
  author  = {Simon Kornblith and Mohammad Norouzi and Honglak Lee and Geoffrey E. Hinton},
  journal = {ICML},
  year    = {2019}
}

@article{dong2023simmtm,
  title   = {SimMTM: A Simple Pre-Training Framework for Masked Time-Series Modeling},
  author  = {Dong, Jiaxiang and Wu, Haixu and Zhang, Haoran and Zhang, Li and Wang, Jianmin and Long, Mingsheng},
  journal = {arXiv preprint arXiv:2302.00861},
  year    = {2023}
}

@article{dao2022flashattention,
  title   = {Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author  = {Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal = {NeurIPS},
  year    = {2022}
}

@article{salinas2020deepar,
  title     = {DeepAR: Probabilistic forecasting with autoregressive recurrent networks},
  author    = {Salinas, David and Flunkert, Valentin and Gasthaus, Jan and Januschowski, Tim},
  journal   = {International Journal of Forecasting},
  volume    = {36},
  number    = {3},
  pages     = {1181--1191},
  year      = {2020},
  publisher = {Elsevier}
}

@article{tolstikhin2021mlp,
  title   = {Mlp-mixer: An all-mlp architecture for vision},
  author  = {Tolstikhin, Ilya O and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and others},
  journal = {NeurIPS},
  year    = {2021}
}

@article{han2023capacity,
  title   = {The Capacity and Robustness Trade-off: Revisiting the Channel Independent Strategy for Multivariate Time Series Forecasting},
  author  = {Han, Lu and Ye, Han-Jia and Zhan, De-Chuan},
  journal = {arXiv preprint arXiv:2304.05206},
  year    = {2023}
}

@article{liu2023koopa,
  title   = {Koopa: Learning Non-stationary Time Series Dynamics with Koopman Predictors},
  author  = {Liu, Yong and Li, Chenyu and Wang, Jianmin and Long, Mingsheng},
  journal = {arXiv preprint arXiv:2305.18803},
  year    = {2023}
}